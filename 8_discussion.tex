\section{Discussion}

\subsection{Theoretical analysis}
In this paper the significance measure is not used heuristically, as it is sometimes used in other literature.
The significance measure is an exact measure of the relative size of gaps when using the naive toolpathing method.
See \cref{naive_overfill_underfill}
Note also that contrary to some literature we don't remove bones from the skeleton, but leave it intact.
The final skeleton is still an exact and full feature descriptor.
In that sense our framework provides an exact solution space.

Note also that our method is robust against small perturbations in the input shape.
Although the skeleton might acquire extra bones, the significant portions of the skeleton remain similar and the output toolpaths change infinitesimally.

Our framework is also local.
The toolpathing around some section of the input polygons is independent of far away regions of the polygon.
This means the toolpathing is stable against large perturbations of the input shape in far away regions
and it also allows for some parallelism during the computation of the toolpaths.


\subsection{Computation time}
The algorithmic complexity is limited by the generation of the Voronoi Diagram, which is $O(n \log n)$, where $n$ is the number of vertices in the input shape.
The number of elements in the trapezoidation is linear in the number of elements in the VD
and all of the stages of our framework are also linear in the number of elements in the skeleton,
so the total running time of our algorithm is $O(n \log n)$.

From the experimental results shows in \cref{computime} we can see that both our framework and the naive method as implemented using Clipper have an expected running time of approximately $10^{-5} n \log n$ seconds.
The computation time is approximately five times the computation time of the naive method, which puts it in the same order of magnitude.

% only Moessen reports on computation times, but in a weird manner



\subsection{Comparison of beading strategies}
\todo{Discussion of finding of Validation section}
The naive strategy and the single bead strategy are of little use to FDM printers, but they show the flexibility of our parametric system.

We can see from \cref{TEST_naive_accuracy}(top) and~\ref{over_underfill} that the naive method causes a lot of overfills and underfills: on average approximately \SI{10}{\percent} of the total target area is covered by underfill and likewise for overfill.
These defects surely influence the mechanical properties of the parts beyond a point that process parameters can reliably be correlated to mechanical properties of the part.

%The single bead strategy solves the underfill problem in regions where the model is more thin than the nozzle size.
%However, the strategy doesn't deal with the remaining area.
%In our visualization we print the outline of the remaining area which exposes an overextrusion problem.

The constant bead count beading strategy effectively deals with all overfills and underfills, but at the cost of wildly differing bead widths.
See \cref{TEST_Constant_accuracy}.
Note that around regions where the bead width changes a lot we can see some overfill areas, which explains the unusually high overfill statistic in \cref{over_underfill}.
Because the bead count 
For an input outline shape which contains both very small and very large features the algorithm produces bead widths which can fall outside of the range of manufacturable bead widths.
Moreover the skeleton marking is not robust against small perturbations in the outline; adding a small chamfer in a corner causes the unmarked skeleton to be very small at that location, which results in tiny bead widths.

If we look at \cref{TEST_Center_accuracy} we can see that
the centered beading strategy effectively deals with overfill and underfill and produces bead widths which are the optimal bead width in all locations, but in the center the bead widths are within a factor 2 off from the optimal bead width, which is still quite a bit.
According to \cref{over_underfill} the overfill and underfill of the over- and underfill for the centered, the evenly distributed and the inward distributed strategy are all approximately \SI{2}{\percent}, which is a considerable improvement over the naive method.

However, according to \cref{widthHistogram} the centered strategy exhibits a lot wider range of bead widths than the distributed strategies:
the standard deviation of the bead widths in the centered beading strategy is approximately \SI{38}{\micro\meter}, while the standard deviation resulting from the distributed strategies is approximately \SI{15}{\micro\meter}.
We can therefore state that the distributed strategies result in bead widths closer to the nominal bead widths than the centered strategy, which means that the distributed strategies realize the mechanical properties associated to a given nominal bead width more closely.

The inward distributed beading strategy was introduced in order to limit the number of beads deviating from the nominal bead width and to limit the number of angled segments in transitioning regions.
The effect is difficult to see when comparing \cref{TEST_Distributed_accuracy} with~\ref{TEST_InwardDistributed_accuracy}, because there are no large solid areas in that specific shape.
Also when looking at \cref{smoothness} we cannot conclude any difference in the smoothness between the evenly distributed and the inward distributed beadign strategy.
We do not provide any statistical evidence that the inward distributed beading strategy is in fact better than the evenly distributed strategy.
However, we can say that by looking at \cref{distributed_comparison} we see that the outer toolpaths have the nominal bead width more often, which means that the inaccuracies of the adaptive bead width deposition system affect the dimensional accuracy of the outline shape less.
The errors from such inaccuracies and the errors in mechanical properties due to deviation from the nominal bead width are located near the center of the shape, which means the overal mechanical properties of the part are least affected by the discrepancy between the actual feature size and the preferred feature size when using the inward distributed beading strategy.






